{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in python \n",
    "\n",
    "## Sommaire : \n",
    "\n",
    "1. [Introduction](#1)<br>\n",
    "    * [Main ML algorithm](#2)<br>\n",
    "    * [Main libraries to work on ML with Python](#3)<br>\n",
    "    * [Differences between supervised and Unsupervised ML](#4)<br>\n",
    "2. [Regression Models](#5)<br>\n",
    "    * [Simple linear regression](#6)<br> \n",
    "    * [Model Evaluation in Regression Models](#7)<br> \n",
    "    * [Evaluation Metrics in Regression Models](#8)<br>\n",
    "    * [Multiple Linear Regression](#9)<br>\n",
    "    * [NON Linear Regression](#10)<br>\n",
    "3. [Classifications Models](#11)<br> \n",
    "    * [K-nearest neighboors](#12)<br>\n",
    "    * [Evaluation metrics in classifications](#13)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id=\"1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main ML algorithm : <a id=\"2\"></a>\n",
    "\n",
    "* <span style=\"color:blue\">***The Regression/Estimation***</span> technique is used for predicting a continuous value. For example, predicting things like the price of a house based on its characteristics, or to estimate the Co2 emission from a car’s engine.\n",
    "* <span style=\"color:blue\">__A Classification__</span> technique is used for Predicting the class or category of a case, for example, if a cell is benign or malignant, or whether or not a customer will churn. \n",
    "* <span style=\"color:blue\">__Clustering__ </span>groups of similar cases, for example, can find similar patients, or can be used for customer segmentation in the banking field. \n",
    "* <span style=\"color:blue\">__Association__ </span>technique is used for finding items or events that often co-occur, for example, grocery items that are usually bought together by a particular customer. \n",
    "* <span style=\"color:blue\">__Anomaly__</span> detection is used to discover abnormal and unusual cases, for example, it is used for credit card fraud detection.\n",
    "* <span style=\"color:blue\">__Sequence mining__ </span>is used for predicting the next event, for instance, the click-stream in websites. \n",
    "* <span style=\"color:blue\">__Dimension reduction__ </span>is used to reduce the size of data. \n",
    "* <span style=\"color:blue\">__Recommendation systems__</span>, this associates people's preferences with others who have similar tastes, and recommends new items to them, such as books or movies\n",
    "\n",
    "### Main libraries to work on ML with Python : <a id=\"3\"></a>\n",
    "\n",
    "* <span style=\"color:blue\"> **NumPy** </span> which is a math library to work with N-dimensional arrays in Python. It enables you to do computation efficiently and effectively. It is better than regular Python because of its amazing capabilities. For example, for working with arrays, dictionaries, functions, datatypes and working with images you need to know NumPy. \n",
    "* <span style=\"color:blue\"> **SciPy** </span> is a collection of numerical algorithms and domain specific toolboxes, including signal processing, optimization, statistics and much more. SciPy is a good library for scientific and high performance computation. \n",
    "* <span style=\"color:blue\"> **Matplotlib** </span> is a very popular plotting package that provides 2D plotting, as well as 3D plotting. \n",
    "* <span style=\"color:blue\"> **Pandas** </span>library is a very high-level Python library that provides high performance easy to use data structures. It has many functions for data importing, manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and timeseries. \n",
    "* <span style=\"color:blue\"> **SciKit Learn** </span> is a collection of algorithms and tools for machine learning which is our focus here and which you'll learn to use within this course\n",
    "\n",
    "\n",
    "### Differences between supervised and Unsupervised ML <a id=\"4\"></a>\n",
    "\n",
    "* <span style=\"color:blue\"> **Supervised** </span> How do we supervise a machine learning model? We do this by __teaching the model__, that is we load the model with knowledge so that we can have it predict future instances. We teach the model by training it with some data from a labeled dataset. **Labeled** means that we already have the results in the dataset.  \n",
    "**Glossary** : *attributes* corespond to columns name, *features* correspond to columns content, and *observations* corespond to the rows of the dataset. *Categorical data* correspond to that is its non-numeric because it contains characters rather than numbers. *Numeric data* is numerical.  Cf below example    \n",
    "2 types of supervised techniques : **Classification & regression**\n",
    "\n",
    "* <span style=\"color:blue\"> **Unsupervised** </span>: We do not supervise the model, but we let the model work on its own to discover information that may not be visible to the human eye. It means, the unsupervised algorithm trains on the dataset, and draws conclusions on unlabeled data. Generally speaking, it is more complicated to implement as we know little about the data and the expected outcomes. The dataset is **not labeled**\n",
    "Main types of Unsupervised techniques : **Dimension reduction, density estimation, market basket analysis, and clustering**. *Dimensionality reduction*, and/or feature selection, play a large role in this by reducing redundant features to make the classification easier.   \n",
    "*Market basket analysis* is a modeling technique based upon the theory that if you buy a certain group of items, you're more likely to buy another group of items.  \n",
    "*Density estimation* is a very simple concept that is mostly used to explore the data to find some structure within it.   \n",
    "*Clustering* is considered to be one of the most popular unsupervised machine learning techniques used for grouping data points, or objects that are somehow similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sup_unsup_comparison](supervised_unsupervised.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class items here is the labeled column, it may be used to classify future tumors as benign or malign. \\nhere this labeled column is considered as a categorical data at the oposite of a numerical data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(np.array([[1,2,3,4,\"benin\"],[4,5,9,5,\"malin\"],[7,5,9,4,\"benin\"]]),columns=['Clump','UniSize','UniShape','Mit','Class'])\n",
    "df\n",
    "#Columns' name correspond to the attributes \n",
    "#Column's content correspond to the features of the dataset\n",
    "# A row corresponds to an observation \n",
    "\"\"\"class items here is the labeled column, it may be used to classify future tumors as benign or malign. \n",
    "here this labeled column is considered as a categorical data at the oposite of a numerical data\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Models <a id=\"5\"></a> \n",
    "\n",
    "In regression, there are two types of variables, a **dependent variable** (convention Y), and one or more **independent variables** (convention X). The dependent variable, can be seen as the state, target, or final goal we study, and try to predict, and the independent variables, also known as explanatory variables, can be seen as the causes of those states. The important point is than dependent variable must be continuous and not discrete values in order to apply regression models. Independent varaibles can be categorical or continuous variable. \n",
    "\n",
    "* Simple regression (one independent variable one dependent variable): \n",
    "    * Simple Linear regression \n",
    "    * Simple non linear regression\n",
    "* Multiple regression (several independent variables): \n",
    "    * Multiple linear regression \n",
    "    * Multiple non linear regression \n",
    " \n",
    " __Here are the differents regression algorithms. Each of them has its own importance, and a specific condition to which their application is best suited__: \n",
    " * Ordinal regression \n",
    " * Poisson regression \n",
    " * fast forest quantile regression \n",
    " * Linear, polynomial, Lasso, Stepwise, Ridge regression \n",
    " * Bayesian linear regression \n",
    " * Neural network regression \n",
    " * Decision forest regression \n",
    " * Boosted decision tree regression \n",
    " * KNN (K-nearest neighbors) \n",
    " \n",
    " ### Simple linear regression <a id=\"6\"></a> \n",
    " \n",
    " `yhat= theta + theta1* X1`\n",
    " * yhat is the dependent variable we want to predict \n",
    " * X is the independent variable \n",
    " * theta is the intercept \n",
    " * theta1 is the he slope or gradient\n",
    " \n",
    " You can interpret this equation as y hat being a function of x1, or y hat being dependent of x1.\n",
    " \n",
    "![calcul_simple_lin_reg](SimpleLinregression-Calcul.JPG) \n",
    "\n",
    " ### Model Evaluation in Regression Models <a id=\"7\"></a> \n",
    " \n",
    " *  <span style=\"color:blue\"> **Training & testing on the same dataset :**   </span>\n",
    "    *  **Training accuracy**: Training accuracy is the percentage of correct predictions that the model makes when using the test dataset. However, a high training accuracy isn't necessarily a good thing. For instance, having a high training accuracy may result in an over-fit the data. This means that the model is overly trained to the dataset, which may capture noise and produce a non-generalized model.   \n",
    "    *  **Out of sample accuracy**: Out-of-sample accuracy is the percentage of correct predictions that the model makes on data that the model has not been trained on. It's important that our models have high out-of-sample accuracy because the purpose of our model is, of course, to make correct predictions on unknown data.\n",
    "    \n",
    " \n",
    " * <span style=\"color:blue\"> **Train & Test split :** </span>\n",
    " Train/test split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive.\n",
    " The issue with train/test split is that it's highly dependent on the datasets on which the data was trained and tested. \n",
    " The variation of this causes train/test split to have a better out-of-sample prediction than training and testing on the same dataset, but it still has some problems due to this dependency.\n",
    " \n",
    " \n",
    " * <span style=\"color:blue\"> **K-Fold Cross validation :** </span>: K-fold cross-validation in its simplest form performs multiple train/test splits, using the same dataset where each split is different. Then, the result is average to produce a more consistent out-of-sample accuracy. \n",
    " \n",
    " ![K_fold_validation](Kfold.JPG) \n",
    "\n",
    "\n",
    "### Evaluation Metrics in Regression Models <a id=\"8\"></a>\n",
    "\n",
    "Evaluation metrics are used to explain the performance of a model.  In the context of regression, the error of the model is the difference between the data points and the trend line generated by the algorithm. Since there are multiple data points, an error can be determined in multiple ways. \n",
    "\n",
    "* <span style=\"color:blue\"> **Mean absolute error** </span> Mean absolute error is the mean of the absolute value of the errors. This is the easiest of the metrics to understand, since it's just the average error.\n",
    "* <span style=\"color:blue\"> **Mean squared error** </span> Mean squared error is the mean of the squared error. It's more popular than mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\n",
    "* <span style=\"color:blue\"> **Root mean squared error.** </span> Root mean squared error is the square root of the mean squared error. This is one of the most popular of the evaluation metrics because root mean squared error is interpretable in the same units as the response vector or y units, making it easy to relate its information. \n",
    "* <span style=\"color:blue\"> **Relative absolute error.** </span> Relative absolute error, also known as residual sum of square, where y bar is a mean value of y, takes the total absolute error and normalizes it by dividing by the total absolute error of the simple predictor. Relative squared error is very similar to relative absolute error but is widely adopted by the data science community, as it is used for calculating R squared.\n",
    "* <span style=\"color:blue\"> **R squared.** </span> R squared is not an error per se but is a popular metric for the accuracy of your model. It represents how close the data values are to the fitted regression line. The higher the R-squared, the better the model fits your data. \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuel=pd.read_csv(\"FuelConsumptionCo2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuel=df_fuel[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_CITY','FUELCONSUMPTION_HWY','FUELCONSUMPTION_COMB','CO2EMISSIONS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour spliter un dataFrame en deux portions test & train exclusives, on peut utiliser plusieurs méthodes dont : \n",
    "\"\"\"Lets split our dataset into train and test sets, 80% of the entire data for training, and the 20% for testing.\n",
    "We create a mask to select random rows using np.random.rand() function: \"\"\"\n",
    "msk = np.random.rand(len(df_fuel)) < 0.8\n",
    "train = df_fuel[msk]\n",
    "test = df_fuel[~msk]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression <a id=\"9\"></a>\n",
    "\n",
    " Adding too many independent variables without any theoretical justification may result in an overfit model. An overfit model is a real problem because it is too complicated for your data set and not general enough to be used for prediction. So, __it is recommended to avoid using many variables for prediction__. \n",
    " \n",
    "  Basically, __categorical independent variables can be incorporated into a regression model by converting them into numerical variables__. For example, given a binary variables such as car type, the code dummy zero for manual and one for automatic cars. \n",
    "  \n",
    "  As a last point, remember that __multiple linear regression is a specific type of linear regression__. So, __there needs to be a linear relationship__ between the dependent variable and each of your independent variables.  There are a number of ways to check for linear relationship. For example, __you can use scatter plots and then visually checked for linearity._ If the relationship displayed in your scatter plot is not linear, then you need to use non-linear regression\n",
    "\n",
    "### NON Linear Regression <a id=\"10\"></a>\n",
    "\n",
    "Sometimes, the trend of data is not really linear, and looks curvy. In this case we can use Polynomial regression methods. In fact, many different regressions exist that can be used to fit whatever the dataset looks like, such as quadratic, cubic, and so on, and it can go on and on to infinite degrees.\n",
    "\n",
    "In essence, we can call all of these, polynomial regression, where the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x.\n",
    "\n",
    "**How can I know if a problem is linear or non-linear in an easy way?**\n",
    "To answer this question, we have to do two things:\n",
    "1. The first is to visually figure out if the relation is linear or non-linear. **It's best to plot bivariate plots of output variables with each input variable.** Also, you can **calculate the correlation** coefficient between independent and dependent variables, and **if, for all variables, it is 0.7 or higher, there is a linear tendency** and thus, it's not appropriate to fit a non-linear regression. \n",
    "2. The second thing we have to do is to use non-linear regression instead of linear regression when we cannot accurately model the relationship with linear parameters. \n",
    "**The second important question is, how should I model my data if it displays non-linear on a scatter plot?**  \n",
    "Well, to address this, you have to use either a polynomial regression, use a non-linear regression model, or transform your data, which is not in scope for this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penser aussi au commentaire sur le plot avec la ligne pleine rouge '-r'\n",
    "#write your code here\n",
    "#1 creation d'un train and test data set \n",
    "\n",
    "#2 création des indepedents variables & dependants variable on train & test data set \n",
    "#3 création des ploynomial features sur train & test independants variables \n",
    "#4 création puis entrainement de la regression lineaire sur les varibales train \n",
    "#5 prediction sur test \n",
    "#6 calcul Mean Absolute error(moyenne ecart absolus), MSE(moyenne écarts au carré) et R2 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifications Models <a id=\"11\"></a> \n",
    "\n",
    " In machine learning classification is a supervised learning approach which can be thought of as a means of categorizing or classifying some unknown items into a discrete set of classes.\n",
    "Classification attempts to learn the relationship between a set of feature variables and a target variable of interest. The target attribute in classification is a categorical variable with discrete values. \n",
    "\n",
    "Here we have the types of classification algorithms and machine learning :\n",
    "* Decision trees\n",
    "* Naive bayes \n",
    "* Linear discriminant analysis \n",
    "* K-nearest neighboor\n",
    "* Logistic regression \n",
    "* Neural networks \n",
    "* Support vector machines.   \n",
    "There are many types of classification algorithms. \n",
    "\n",
    "### K-nearest neighboors <a id=\"12\"></a>\n",
    "\n",
    "The K-Nearest Neighbors algorithm is a classification algorithm that **takes a bunch of labeled points and uses them to learn how to label other points**. This algorithm classifies cases based on their **similarity to other cases**. In K-Nearest Neighbors, data points that are near each other are said to be neighbors. \n",
    "\n",
    "K-Nearest Neighbors is based on this paradigm. ***Similar cases with the same class labels are near each other***\n",
    "\n",
    "![KNN](KNN.JPG)\n",
    "\n",
    "In a classification problem, the K-Nearest Neighbors algorithm works as follows: \n",
    "1. <span style='color:blue'>Pick a value for K </span>\n",
    "2. <span style='color:blue'>Calculate the distance</span> from the new case hold out from each of the cases in the dataset \n",
    "3. <span style='color:blue'>Search for the K-observations</span> in the training data that are nearest to the measurements of the unknown data point\n",
    "4. <span style='color:blue'>Predict the response </span> of the unknown data point using the most popular response value from the K-Nearest Neighbors. \n",
    "\n",
    "In order to <span style='color:blue'>**calculate the distance**</span>, we can use an euclidiean distance : \n",
    "![Distance](distance.JPG)\n",
    "\n",
    " Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used for this purpose but as mentioned, it is highly dependent on datatype and also the domain that classification is done for it. \n",
    " \n",
    " <span style='color:blue'>**Determine best value for K**</span>   \n",
    "K=1 is not a good solution as it captures noises and the model would be overfitting which is bad. \n",
    "if k is too high, it will lead to a too generalised model and predictions will not be accurate \n",
    "\n",
    "The best method is **to use the test set to calculate the accuracy of your model**.  \n",
    "Example :Choose K equals one and then use the training part for modeling and calculate the accuracy of prediction using all samples in your test set. Repeat this process increasing the K and see which K is best for your model. For example, in our case, K equals four will give us the best accuracy.\n",
    "\n",
    "### Evaluation metrics in classifications <a id=\"13\"></a>\n",
    "\n",
    "There are different model evaluation metrics but we just talk about three of them here, specifically: \n",
    "\n",
    "* <span style='color:blue'>Jaccard index or Jaccard similarity coefficient </span>: Then we can define Jaccard as the size of the intersection divided by the size of the union of two label sets. For example, for a test set of size 10, with 8 correct predictions, or 8 intersections, the accuracy by the Jaccard index would be 0.66. \n",
    "![Jaccard](jaccard.JPG)\n",
    "\n",
    "* <span style='color:blue'>F1-score, </span>\n",
    "In the specific case of a binary classifier, we can interpret these numbers as the count of true positives <span style='color:orange'> **TP** </span>, false negatives<span style='color:orange'>**FN** </span>, true negatives<span style='color:orange'> **TN** </span>, and false positives<span style='color:orange'> **FP** </span>.   \n",
    "**Precision** is a measure of the accuracy, provided that a class label has been predicted. It is defined by: precision = True Positive / (True Positive + False Positive). And **Recall** is the true positive rate. It is defined as: Recall = True Positive / (True Positive + False Negative)  \n",
    "***The F1 score*** is the harmonic average of the precision and recall, where an F1 score reaches **its best value at 1**(which represents perfect precision and recall) and its worst at 0. \n",
    "F1 score can also be used on a multiclass classifier.\n",
    "![f1](f1.JPG)\n",
    "  \n",
    "* <span style='color:blue'>Log Loss. </span> Sometimes, the output of a classifier is the probability of a class label, instead of the label. For example, in logistic regression, the output can be the probability of customer churn, i.e., yes (or equals to 1). This probability is a value between 0 and 1. Logarithmic loss (also known as Log loss) measures the performance of a classifier where the predicted output is a probability value between 0 and 1. So, for example, predicting a probability of 0.13 when the actual label is 1, would be bad and would result in a high log loss. We can calculate the log loss for each row using the log loss equation, which measures how far each prediction is, from the actual label. Then, we calculate the average log loss across all rows of the test set. It is obvious that more ideal classifiers have progressively smaller values of log loss. So, the classifier with lower log loss has better accuracy.\n",
    "![logloss](log.JPG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
